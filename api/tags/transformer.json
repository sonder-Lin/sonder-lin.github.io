{"name":"transformer","slug":"transformer","count":1,"postlist":[{"title":"注意力机制","uid":"355624fb0490d5f1612ed79605ab0b34","slug":"attention","date":"2024-09-22T05:51:09.000Z","updated":"2024-09-22T12:55:12.290Z","comments":true,"path":"api/articles/attention.json","keywords":null,"cover":[],"text":"Attention自注意力机制Self-attention注意力机制在设计之初的目标是接收一段文本，预测其下一个词。 首先，输入文本会被切分成小块，称之为tok...","permalink":"/post/attention","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"深度学习","slug":"深度学习","count":4,"path":"api/categories/深度学习.json"}],"tags":[{"name":"attention","slug":"attention","count":1,"path":"api/tags/attention.json"},{"name":"transformer","slug":"transformer","count":1,"path":"api/tags/transformer.json"}],"author":{"name":"Sonderlin","slug":"blog-author","avatar":"https://raw.githubusercontent.com/sonder-lin/Mypic/img/img/%E5%A4%B4%E5%83%8F.jpg","link":"/","description":"一位正在重塑知识的小蒟蒻~ <br /> <b><i>且视他人之疑目如盏盏鬼火，大胆地去走你的夜路。</i></b> <br /><br /> @ <b>qq：1425906813</b>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}]}