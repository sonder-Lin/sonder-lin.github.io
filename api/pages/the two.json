{"title":"","type":"page","uid":"1cd19fc71517798e2fa5b7457d1fb06e","text":"文献阅读汇报Towards deep learning models resistant to adversarial attacks(PGD)Towards Evaluating the Robustness of Neural Networks(C&W) 1.Towards ...","date":"2024-09-13T09:01:46.405Z","updated":"2024-09-12T07:16:45.479Z","comments":true,"path":"api/pages/the two.json","covers":null,"excerpt":"","content":"<h1 id=\"文献阅读汇报\"><a href=\"#文献阅读汇报\" class=\"headerlink\" title=\"文献阅读汇报\"></a>文献阅读汇报</h1><h3 id=\"Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD\"><a href=\"#Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD\" class=\"headerlink\" title=\"Towards deep learning models resistant to adversarial attacks(PGD)\"></a>Towards deep learning models resistant to adversarial attacks(PGD)</h3><h3 id=\"Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W\"><a href=\"#Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W\" class=\"headerlink\" title=\"Towards Evaluating the Robustness of Neural Networks(C&amp;W)\"></a>Towards Evaluating the Robustness of Neural Networks(C&amp;W)</h3><hr>\n<h3 id=\"1-Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD-论文解读\"><a href=\"#1-Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD-论文解读\" class=\"headerlink\" title=\"1.Towards deep learning models resistant to adversarial attacks(PGD)论文解读\"></a>1.Towards deep learning models resistant to adversarial attacks(PGD)论文解读</h3><h3 id=\"论文概要\"><a href=\"#论文概要\" class=\"headerlink\" title=\"论文概要\"></a>论文概要</h3><p>本文即在FGSM的基础上进行了改进，针对FGSM仅做一次迭代的缺陷（难以应对非线性的损失函数），提出了PGD迭代攻击方法，即K-FGSM（K表示迭代的次数），做K次FGSM，并使得每次的幅度更小。即从一次一大步改进为多次一小步。<br>$$<br>x_{t+1} &#x3D; \\prod_{x+S} \\left( x_t + \\alpha \\cdot \\text{sign}(\\nabla_x J(x_t, y)) \\right)<br>$$<br>好像还存在L2范数归一化的变式，避免了sign没有归一化的缺点：<br>$$<br>\\mathbf{X}<em>{t+1} &#x3D; \\Pi</em>{X+S} \\left( \\mathbf{X}_t + \\epsilon \\left( \\frac{g_t}{|g_t|} \\right) \\right)<br>$$</p>\n<hr>\n<h3 id=\"2-Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W-论文解读\"><a href=\"#2-Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W-论文解读\" class=\"headerlink\" title=\"2.Towards Evaluating the Robustness of Neural Networks(C&amp;W)论文解读\"></a>2.Towards Evaluating the Robustness of Neural Networks(C&amp;W)论文解读</h3><h3 id=\"论文概要-1\"><a href=\"#论文概要-1\" class=\"headerlink\" title=\"论文概要\"></a>论文概要</h3>","count_time":{"symbolsCount":632,"symbolsTime":"1 mins."},"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%B1%87%E6%8A%A5\"><span class=\"toc-text\">文献阅读汇报</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD\"><span class=\"toc-text\">Towards deep learning models resistant to adversarial attacks(PGD)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W\"><span class=\"toc-text\">Towards Evaluating the Robustness of Neural Networks(C&amp;W)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-Towards-deep-learning-models-resistant-to-adversarial-attacks-PGD-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB\"><span class=\"toc-text\">1.Towards deep learning models resistant to adversarial attacks(PGD)论文解读</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E6%A6%82%E8%A6%81\"><span class=\"toc-text\">论文概要</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-Towards-Evaluating-the-Robustness-of-Neural-Networks-C-W-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB\"><span class=\"toc-text\">2.Towards Evaluating the Robustness of Neural Networks(C&amp;W)论文解读</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E6%A6%82%E8%A6%81-1\"><span class=\"toc-text\">论文概要</span></a></li></ol></li></ol></li></ol>","data":[]}