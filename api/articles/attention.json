{"title":"注意力机制","uid":"355624fb0490d5f1612ed79605ab0b34","slug":"attention","date":"2024-09-22T05:51:09.000Z","updated":"2024-09-29T06:07:49.129Z","comments":true,"path":"api/articles/attention.json","keywords":null,"cover":[],"content":"<h1 id=\"Attention\"><a href=\"#Attention\" class=\"headerlink\" title=\"Attention\"></a>Attention</h1><h3 id=\"自注意力机制Self-attention\"><a href=\"#自注意力机制Self-attention\" class=\"headerlink\" title=\"自注意力机制Self-attention\"></a>自注意力机制Self-attention</h3><p>注意力机制在设计之初的目标是接收一段文本，预测其下一个词。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922193728398.png\" alt=\"image-20240922193728398\" style=\"zoom: 33%;\">\n\n<p>首先，输入文本会被切分成小块，称之为token（可能由单个单词或者多个单词组成），即一个句子的一个小单位</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922193823929.png\" alt=\"image-20240922193823929\" style=\"zoom:33%;\">\n\n<p>接着，在transformer模型中，我们将之前划分的每一个token关联到一个称为嵌入向量（embedding）的高维向量中，而在高维空间中向量的方向即可表示对应的语义信息。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922194112186.png\" alt=\"image-20240922194112186\" style=\"zoom:33%;\">\n\n<p>而transformer模型的目标即是 在将一个文本句子编码成一个个单个词后，通过逐步调整对应词的嵌入向量（embeddings） 来融入对应词的上下文信息（语义信息）。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922194836306.png\" alt=\"image-20240922194836306\" style=\"zoom: 33%;\">\n\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922194902463.png\" alt=\"image-20240922194902463\" style=\"zoom: 80%;\">\n\n<p><strong>而如何逐步调整文本句子中每一个词对应的嵌入向量（embeddings）以融入语义信息则是transformer模型的核心——注意力机制。</strong></p>\n<p>对于理解这个问题，可以以一个这样的例子进行理解：假设现在一个句子中的语义信息只存在于形容词对名词的作用。</p>\n<p>a fluffy blue creature roamed the verdant forest</p>\n<p>因此，可以想象成每个名词，此处以creature为例，向前面的词发出询问：我的前面有形容词吗？显然，fluffy和blue这两个词想回应：是的，我是你前面的形容词。</p>\n<p>上述的提问在注意力机制中被编码为一个向量，称为对应词的query向量，而要计算这个query向量则需要先取一个矩阵，即为<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.65ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.589ex\" height=\"2.195ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1586.3 970.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g></g></g></g></svg></mjx-container>，与对应词embedding进行矩阵相乘得到对应词的query向量。</p>\n<p>其中，<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.65ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.589ex\" height=\"2.195ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1586.3 970.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g></g></g></g></svg></mjx-container>被称为查询矩阵，其中的元素为模型需要去学习的参数，形状为128x12288（相当于将embedding降维至128维了）。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922195728247.png\" alt=\"image-20240922195728247\" style=\"zoom: 33%;\">\n\n<p>此外，在对提问定义后，那上述所谓“形容词的回答”该如何定义呢？因此我们引入了第二个待学习参数矩阵——<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.157ex\" height=\"1.902ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1395.4 840.8\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g></g></svg></mjx-container>键矩阵，同样与embedding相乘得到键向量<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.011ex\" height=\"1.545ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 889 683\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g></g></g></svg></mjx-container>,与查询矩阵相同，键矩阵同样将embedding降维至128维的低维空间。</p>\n<p>当查询query向量与键key向量的方向接近时，我们就能认为它们相匹配，即可将其当作对查询query的积极回应，从而我们通过计算每个词对应query、key向量的点积，根据点积的大小即可，判断query与key的匹配程度。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922200952510.png\" alt=\"image-20240922200952510\" style=\"zoom: 33%;\">\n\n<p>而这些匹配程度对应值的大小，我们后续将用来作为权重，因此需要介于0-1，因此，我们对于每一列应用softmax函数进行归一化，从而得到权重：</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922201156887.png\" alt=\"image-20240922201156887\" style=\"zoom: 33%;\">\n\n\n\n<p>在训练transformer模型时，我们可以通过将文本信息根据token拆分成多个子集，并行地在模型上对其进行训练</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922201613465.png\" alt=\"image-20240922201613465\" style=\"zoom:33%;\">\n\n<p>但为了保障这种并行性，意味着不能让后面词的存在影响前面句子的预测，不然会泄露，因此在训练时需要保证模型只能已知前面的词，也就是权重左下角在softmax层运算前全设为负无穷，得到的最终softmax输出权重即为0。<strong>这种机制即掩码（masking）机制。</strong></p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922201936703.png\" alt=\"image-20240922201936703\" style=\"zoom: 33%;\">\n\n\n\n<p>接着，我们回到如何调整embedding能嵌入语义信息这一问题，为了实现这一目标，我们将用到第三个矩阵：值矩阵<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.584ex\" height=\"1.359ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -443 1141.9 600.8\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D464\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(749,-150) scale(0.707)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\"></path></g></g></g></g></svg></mjx-container>，将其乘以前面词的embedding，即可得到前面一个词对当前词的值向量Value，即要给当前词的embedding要加上的向量，从而实现对当前词embedding的调整，即融入前一个词对其的语义影响。</p>\n<p>显然，这个值向量与词embedding处于同一维度，因此<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.357ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.157ex\" height=\"1.902ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1395.4 840.8\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g></g></svg></mjx-container>的形状为12288x12288</p>\n<p>因此，我们计算任意两个词之间的value，使用之前的权重进行加权求和，得到对应embe-dding需要加上的量（调整方向，融入语义信息）</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922202817041.png\" alt=\"image-20240922202817041\" style=\"zoom: 33%;\">\n\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922202920424.png\" alt=\"image-20240922202920424\" style=\"zoom: 33%;\">\n\n<p><strong>以上即为单头自注意力机制的原理，其由三个待学习的参数矩阵构成：<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.65ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.073ex\" height=\"1.652ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -443 1358.3 730.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D464\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(749,-150) scale(0.707)\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g></g></g></g></svg></mjx-container>、<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.339ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.23ex\" height=\"1.342ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -443 1427.6 593\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D464\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(749,-150) scale(0.707)\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g></g></g></g></svg></mjx-container>、<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.375ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.038ex\" height=\"1.377ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -443 1342.8 608.6\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D464\" d=\"M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(749,-150) scale(0.707)\"><path data-c=\"1D449\" d=\"M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z\"></path></g></g></g></g></svg></mjx-container></strong></p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922203055934.png\" alt=\"image-20240922203055934\" style=\"zoom:33%;\">\n\n<p>此外，为了保证并行性和运算速度，通常将V矩阵进行分解：</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922203308640.png\" alt=\"image-20240922203308640\" style=\"zoom:33%;\">\n\n<h3 id=\"交叉注意力机制Cross-attention\"><a href=\"#交叉注意力机制Cross-attention\" class=\"headerlink\" title=\"交叉注意力机制Cross-attention\"></a>交叉注意力机制Cross-attention</h3><p>交叉注意力机制与自注意力机制基本相同，唯一的区别在于：<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.339ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.746ex\" height=\"1.885ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1655.6 833\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g></g></g></g></svg></mjx-container>和<mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.65ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"3.589ex\" height=\"2.195ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -683 1586.3 970.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D44A\" d=\"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(977,-150) scale(0.707)\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g></g></g></g></svg></mjx-container>矩阵作用于不同的数据集：</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922203628053.png\" alt=\"image-20240922203628053\" style=\"zoom:33%;\">\n\n<p>例如在机器翻译中，查询来自一种语言，键可能来自另一种语言，即可描述为一种语言中的词对应于其他语言中的哪些词，因此可以发现一个句子中后面的词对前面词的翻译预测不会有影响，<strong>因此不再使用掩码机制（masking）</strong></p>\n<h3 id=\"多头注意力机制Multi-headed-attention\"><a href=\"#多头注意力机制Multi-headed-attention\" class=\"headerlink\" title=\"多头注意力机制Multi-headed attention\"></a>多头注意力机制Multi-headed attention</h3><img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922204011716.png\" alt=\"image-20240922204011716\" style=\"zoom: 33%;\">\n\n<p>即多头，进一步提高并行性，在最后将各头给出的embedding变化量加起来，给原始embedding加上这个和，即可得到一个更精确的调整后的embedding。</p>\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922204231570.png\" alt=\"image-20240922204231570\" style=\"zoom: 33%;\">\n\n<img src=\"https://raw.githubusercontent.com/sonder-Lin/Mypic/img/img/image-20240922204306236.png\" alt=\"image-20240922204306236\" style=\"zoom:33%;\">\n\n\n\n<p>参考视频：<a href=\"https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.337.search-card.all.click&vd_source=2fc3d89c840e069d13993f1514edd1d8\">https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.337.search-card.all.click&amp;vd_source=2fc3d89c840e069d13993f1514edd1d8</a></p>\n<p>（3b1b无敌！）</p>\n","text":"Attention自注意力机制Self-attention注意力机制在设计之初的目标是接收一段文本，预测其下一个词。 首先，输入文本会被切分成小块，称之为tok...","permalink":"/post/attention","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"深度学习","slug":"深度学习","count":4,"path":"api/categories/深度学习.json"}],"tags":[{"name":"attention","slug":"attention","count":1,"path":"api/tags/attention.json"},{"name":"transformer","slug":"transformer","count":1,"path":"api/tags/transformer.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Attention\"><span class=\"toc-text\">Attention</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attention\"><span class=\"toc-text\">自注意力机制Self-attention</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Cross-attention\"><span class=\"toc-text\">交叉注意力机制Cross-attention</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Multi-headed-attention\"><span class=\"toc-text\">多头注意力机制Multi-headed attention</span></a></li></ol></li></ol></li></ol>","author":{"name":"Sonderlin","slug":"blog-author","avatar":"https://raw.githubusercontent.com/sonder-lin/Mypic/img/img/%E5%A4%B4%E5%83%8F.jpg","link":"/","description":"一位正在重塑知识的小蒟蒻~ <br /> <b><i>过了河便是繁星。</i></b> <br /><br /> @ <b>qq：1425906813</b>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"Stable Diffusion","uid":"9f952bd90e9edabc954e41df6aae978e","slug":"Stable-Diffusion","date":"2024-09-22T05:51:09.000Z","updated":"2025-01-05T10:53:26.530Z","comments":true,"path":"api/articles/Stable-Diffusion.json","keywords":null,"cover":[],"text":"Stable DiffusionStable Diffusion 本身并不是一个模型，而是一个由多个模块和模型组成的系统架构，它由三大核心部件组成，每个组件都是...","permalink":"/post/Stable-Diffusion","photos":[],"count_time":{"symbolsCount":508,"symbolsTime":"1 mins."},"categories":[{"name":"深度学习","slug":"深度学习","count":4,"path":"api/categories/深度学习.json"}],"tags":[{"name":"SD扩散模型","slug":"SD扩散模型","count":1,"path":"api/tags/SD扩散模型.json"},{"name":"AIGC","slug":"AIGC","count":2,"path":"api/tags/AIGC.json"}],"author":{"name":"Sonderlin","slug":"blog-author","avatar":"https://raw.githubusercontent.com/sonder-lin/Mypic/img/img/%E5%A4%B4%E5%83%8F.jpg","link":"/","description":"一位正在重塑知识的小蒟蒻~ <br /> <b><i>过了河便是繁星。</i></b> <br /><br /> @ <b>qq：1425906813</b>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Diffusion Model","uid":"7ce7aaea6148fcaf3be1f2eb31902870","slug":"diffusion model","date":"2024-09-21T05:51:09.000Z","updated":"2025-01-05T10:53:43.913Z","comments":true,"path":"api/articles/diffusion model.json","keywords":null,"cover":[],"text":"Diffusion Model原理解析前向扩散过程前向扩散过程本质上为一个对原始图片输入逐步加噪的过程，公式如下所示：其中，表示在时刻的噪音，其服从正态分布：....","permalink":"/post/diffusion model","photos":[],"count_time":{"symbolsCount":632,"symbolsTime":"1 mins."},"categories":[{"name":"深度学习","slug":"深度学习","count":4,"path":"api/categories/深度学习.json"}],"tags":[{"name":"AIGC","slug":"AIGC","count":2,"path":"api/tags/AIGC.json"},{"name":"扩散模型","slug":"扩散模型","count":1,"path":"api/tags/扩散模型.json"}],"author":{"name":"Sonderlin","slug":"blog-author","avatar":"https://raw.githubusercontent.com/sonder-lin/Mypic/img/img/%E5%A4%B4%E5%83%8F.jpg","link":"/","description":"一位正在重塑知识的小蒟蒻~ <br /> <b><i>过了河便是繁星。</i></b> <br /><br /> @ <b>qq：1425906813</b>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}