[{"id":"fe240c0ae0adf7f1b348f04155b49b7b","title":"Evasion attacks against machine learning at test time","content":"Evasion attacks against machine learning at test time论文概述作者通过模拟攻击者逃避机器学习中常用的几类分类器这一场景，在诸多假设的前提下，从攻击者的视角。。。。\n","slug":"newpage","date":"2024-09-01T12:01:23.000Z","categories_index":"论文阅读","tags_index":"机器学习,逃避攻击","author_index":"Sonderlin"},{"id":"43114f1350cad18fe627afc10a1b1c7b","title":"Transcend：Detecting Concept Drift in Malware Classification Models论文阅读总结","content":"Transcend: Detecting Concept Drift in Malware Classification Models出处：USENIX Security Symposium\n时间：2017\n论文概述针对攻击者不断改良恶意软件带来的概念漂移问题，本文在模型性能下降之前就进行对退化的检测模型进行识别，从而及时完成检测模型的更新\n现有研究传统方法识别模型退化：\n\n根据模型检测完成后的准确率等指标后顾性判断\n计算测试对象在候选类别中的拟合概率\n\n解决方案Conformal evaluation（保形评估）置信度（Confidence）、可信度（Credibility）、不一致性度量都是保形评估的核心。\n不一致性度量\n\n其中评分函数AD即核心的不一致性度量函数\n本文中使用p值作为相似性的度量（P-values as a Similarity Metric）：\n\n计算式解释：\n\n\nCredibility（可信度）定义：表示样本与其被分类的类别在统计上的相似程度，即样本属于该类别的可信度。\n\n\n\n\n总结：可信度越高，则表明样本在统计上更接近被分类的类别。但存在多类别匹配的局限，即对于多个类别的p值都很高，因此引入置信度\nConfidence（置信度）定义：表示算法对给定分类决策的确信程度，即算法认为该决策正确的程度\n\n\nps：max括号中的意思是除了已选择的标签的p值，最终得到的是其他标签下的p值最大值\n决策评估\n\n如果一个算法的决策评估状态是出现高可信度且高置信度的，我们就认为这个算法在决策评估上目前还不错，出现其他情况就说明可能发生了概念漂移。\n","slug":"Transcend","date":"2024-08-30T05:51:09.000Z","categories_index":"机器学习","tags_index":"概念漂移,保形预测","author_index":"Sonderlin"},{"id":"9627999a85156d234bd83b92abfc393b","title":"Hexo博客发文/配置/管理常用指令","content":"Hexo博客发文&#x2F;配置&#x2F;管理常用指令powershellhexo init  //初始化\n\nhexo new/n (title)  //新建文章\n\nhexo generate/g  //生成静态文件\n\nhexo server/s   //启动本地服务器 进行预览\n\nhexo clean      //清理缓存，有时更改了设置需要清除缓存后才可生效\n\nhexo deploy    //将本地生成的静态文件部署到github仓库中上线","slug":"Hexo博客常用指令","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客管理命令","author_index":"Sonderlin"},{"id":"de3a9296d5a1bb4a6f357efb2f755731","title":"Hexo博客配置时遇到的问题","content":"Hexo博客配置时遇到的问题问题1：hexo博客代码高亮失效吐血了。。。csdn真一坨XX。。。网上找了半天才发现是hexo的版本高于7.0.0时代码高亮就会失效。\nbashnpm i hexo@6.0.0 //回退版本至6.x.x后成功解决问题2：作者页下面的信息栏统计有误。。一个文章必须要先设置category再设置tags  两者才会统计生效\n问题3：新建文章时没有自带categories字段默认情况下，新创建的文章是没有 categories 和 tags 的，我们可以在根目录下的 scaffolds\\post.md 文件添加即可。这样每次新建文章，就自动有了这些参数。\n终于没问题了！！！准备开始愉快地写博客了！！\n","slug":"Hexo博客配置时遇到的问题","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客配置问题","author_index":"Sonderlin"},{"id":"feee444778bd7ce32a76dc60cad27adf","title":"SHAP算法原理","content":"Shapley值1953年，Shapley值首次提出。Lloyd Shapley在论文《A Value for n-Person Games》中首次提出Shapley值。Shapley值是合作博弈论中的一个解决方案，它提供了一种在玩家之间公平分配支出的数学方法。从此成为联盟博弈论的基石，经常被用来确定团体内公平有效的资源分配策略，包括在股东之间分配利润、在合作者之间分配成本以及将功劳分配给研究项目的贡献者。然而，它与机器学习的交集要到五十年后才有。\nShapley值计算假设以下情形：已经训练了一个机器学习模型来预测公寓价格，分别有park、size、floor、car四个特征。\n某个面积为50平方米（size&#x3D;50）、位于二楼（floor&#x3D;2nd）、附近有一个公园（park&#x3D;nearby）、禁止猫咪（cat&#x3D;banned）的公寓，它预测价格为300,000欧元，你需要解释这个预测，即每个特征是如何促进预测的？当所有公寓的平均预测为310,000欧元时，与平均预测相比，每个特征值对预测的贡献是多少？\n线性回归模型的答案很简单，每个特征的贡献是特征的权重乘以特征值，但这仅适用于线性模型\npark&#x3D;nearby，cat&#x3D;banned，size&#x3D;50，floor&#x3D;2nd的特征值共同实现了300,000欧元的预测，而我们的目标是解释实际预测（300,000欧元）和平均预测（310,000欧元）之间的差异：-10,000欧元。\n答案可能是：park&#x3D;nearby贡献了30,000欧元，size&#x3D;50贡献了10,000欧元，floor&#x3D;2nd贡献了0欧元，cat&#x3D;banned贡献了-50,000欧元，这些贡献加起来为-10,000欧元，最终预测为平均预测加上该贡献之和。\n那么我们该如何计算目标公寓实例各特征的Shapley值呢？\n在下面中，我们估计了cat&#x3D;banned特征值被添加到park&#x3D;nearby和size&#x3D;50的联盟后的贡献。\n第一步，我们从数据中随机抽取另一个公寓（随机找一个floor&#x3D;1st的公寓），使用该公寓自己的floor特征值1st，模拟出park&#x3D;nearby，size&#x3D;50和cat&#x3D;banned的联盟，然后我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;banned）预测公寓的价格为310,000欧元，这个估计值取决于随机抽取的公寓的值，这称之为基线值。\n第二步，我们从联盟中删除cat&#x3D;banned，然后用该公寓的cat特征值allowed替代，我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;allowed）预测公寓的价格为320,000欧元。\n第三步，重复以上过程，floor取另一个值，直到找完，就找完了被添加到park&#x3D;nearby和size&#x3D;50的联盟后的全部贡献\n将以下全部联盟的贡献都找完，计算带有和不带有cat&#x3D;banned特征值的预测公寓价格，并计算差值来获得边际贡献，将边际贡献求出（加权）平均值，从而得出Shapley值。\n \n\n\n\n","slug":"Shapley","date":"2024-07-07T14:15:09.000Z","categories_index":"机器学习","tags_index":"SHAP","author_index":"Sonderlin"},{"id":"977ed608adc923ebbb0d8daa2e5720c8","title":"DOH隧道工具使用","content":"dns2tcp使用教程https://www.jianshu.com/p/9e8367fee777\n文件传输netcat工具安装https://blog.csdn.net/qq_50377269/article/details/135772156\n等待ns记录生效ing\n(开错端口了 开成tcp53了  重新开放udp53就可以了\ngodoh doh隧道工具https://github.com/sensepost/godoh\n","slug":"DOH隧道工具使用","date":"2024-07-07T06:05:49.000Z","categories_index":"DNS","tags_index":"DoH隧道","author_index":"Sonderlin"},{"id":"c723f7d5a5457cbc1c88bbea11b3a805","title":"Pytorch学习","content":"Pytorch学习数据操作tensor为torch中的一个数据结构，可以参与运算，称为张量，即多个数值组成的数组，可能有多个维度\npythonimport torch\nx = torch.arrange(3)  #生成一个从0开始,长度为3的一维tensor\nx\nOut:tensor([0,1,2])\n    \nx.shape                #查看tensor的形状\nOut:torch.Size([3])    #形状为：一维、长度为3\n    \nX=x.reshape(3,4)       #改变一个tensor的形状 改为二维的3x4矩阵\nOut:tensor([[],\n       ....            #一个二维矩阵\n           ])\n\ntorch.cat((tensor1,tensor2),dim=0/1)       #按行或按列 合并tensor\n\n\n#torch的广播机制\n当两个tensor维度相同，但形状不一致进行运算时，两者从低到高扩展然后进行运算\n\n#torch的原地操作（以节省内存）\nZ = torch.zeros_like(Y)   #形状与Y一致，但初值为全0\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n\nid(Z): 140327634811696             \nid(Z): 140327634811696            #运算后地址不变，成功原地操作\n    \n#对于可变对象，如列表、集合、字典等\nX += Y 不等同于 X = X + Y\n前者为原地操作，后者创建了新的对象（浪费了内存）\n对于不可变对象 两者等同\n\n\n#torch与numpy\nx = torch.arrange(3)\nx为一个tensor对象\nA = x.numpy()  #将一个tensor对象转换为numpy中的ndarray对象\nB = torch.tensor(A)  #将一个ndarray对象转换为tensor对象\n\na = torch.tensor([3.5])   #创建一个一维的 大小为1 的张量\na, a.item(), float(a), int(a)    #转换为python中的标量\noutput:(tensor([3.5000]), 3.5, 3.5, 3)线性代数利用代码实现一些线代中的运算\npythonimport torch\nA = torch.arange(20).reshape(5, 4)     #初始化 5x4矩阵\nA\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\nA.T                                    #获取其转置\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n\n#矩阵加法\nA = torch.arange(20, dtype=torch.float32).reshape(5, 4)\nB = A.clone()  # 通过分配新内存，将A的一个副本分配给B\nA, A + B\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))\n\nA * B                        #按元素乘法  \ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\n\n#降维操作\nx = torch.arange(4, dtype=torch.float32)\nx, x.sum()\n(tensor([0., 1., 2., 3.]), tensor(6.))\n\n#默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。\nA_sum_axis0 = A.sum(axis=0)\nA_sum_axis0, A_sum_axis0.shape\n(tensor([40., 45., 50., 55.]), torch.Size([4]))\nA_sum_axis1 = A.sum(axis=1)\nA_sum_axis1, A_sum_axis1.shape\n(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))\n\n#求平均值\nA.mean(), A.sum() / A.numel()\n(tensor(9.5000), tensor(9.5000))\n\n#点积\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\n\n#矩阵-向量积\nA.shape, x.shape, torch.mv(A, x)\n(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))\n\n#矩阵-矩阵乘法\nB = torch.ones(4, 3)\ntorch.mm(A, B)\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n\n#范数\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)             #L2范数：所有元素平方和的平方根\ntensor(5.)\n\ntorch.abs(u).sum()        #L1范数：所有元素的绝对值求和\ntensor(7.)微积分对矩阵进行诸如求导数等运算\n\n\n分子布局符号，以分子优先\n对矩阵求导后的形状：\n\n\n计算图理论\n\n\n由此引出反向传递：\n\n\n\n\n\n\n可以发现反向传播需要利用正向传播求导的中间结果，空间复杂度较高（因此比较占用GPU资源），计算复杂度与正向传播的代价类似，但正向传播不需要存储中间结果，空间复杂度较低，但正是由于其不存储中间结果，因此导致每次都需要重新计算，导致在实际使用时的时间复杂度太高。\n自动求导python import torch\n\nx = torch.arange(4.0)\nx\ntensor([0., 1., 2., 3.])\n\nx.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\nx.grad  # 默认值是None\n\n# 现在计算y\ny = 2 * torch.dot(x, x)         #内积，因此y=2x^2\ny\ntensor(28., grad_fn=&lt;MulBackward0&gt;)\n\n#接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\ny.backward()      #反向传播进行求导\nx.grad            #直接访问y关于x的导数\n\nx.grad == 4 * x\ntensor([True, True, True, True])\n\nx.grad.zero_()         # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n\ny = x.sum()\ny.backward()\nx.grad\ntensor([1., 1., 1., 1.])\n\n#在深度学习中 一般仅对标量进行求导\n\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()     #清空梯度\ny = x * x\n# 等价于y.backward(torch.ones(len(x)))\ny.sum().backward()\nx.grad\n\n\nx.grad.zero_()\ny = x * x\nu = y.detach()     #当做常数\nz = u * x   \n\nz.sum().backward()      \nx.grad == u        #求导时也将其当做了常数\ntensor([True, True, True, True])\n\n\n#控制流的梯度计算\ndef f(a):\n    b = a * 2\n    while b.norm() &lt; 1000:\n        b = b * 2\n    if b.sum() &gt; 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\na.grad == d / a\ntensor(True)线性神经网络线性回归​\t\t线性回归是指目标可以表示为特征的加权和，如下面的式子，其中w代表权重，x代表某样本数据的各特征，b偏移量（所有特征为0时的值）：​       我们可以用点积形式来简洁地表达模型：其中向量x对应于单个数据样本的特征​\t\t对于特征集合X，预测值可以通过矩阵-向量乘法表示为：其中，X的每一行是一个样本，每一列是一种特征。​\t\t在开始寻找最好的模型参数（model parameters）和之前， 我们还需要两个东西：一种模型质量的度量方式；一种能够更新模型以提高模型预测质量的方法。\n损失函数​\t在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。​\t损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。当样本i的预测值为，其相应的真实标签为时， 平方误差可以定义为以下公式：   为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）。   在训练模型时，我们希望寻找一组参数（w∗,b∗）， 这组参数能最小化在所有训练样本上的总损失。如下式：   我们的预测问题是最小化，由于其为凸函数，即在损失平面上只有一个临界点，所以存在参数的解析解。因此将损失关于w的导数设为0，得到解析解：\n   像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。\n随机梯度下降即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。\n​\t\t本书中我们用到一种名为梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向\t\t上更新参数来降低误差。\n​\t\t梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常\t\t慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量\t\t随机梯度下降（minibatch stochastic gradient descent）。\n​       在每次迭代中，我们首先随机抽样一个小批量，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以\t\t称为梯度）。最后，我们将梯度乘以一个预先确定的正数（即学习率），并从当前参数的值中减掉，从而将参数沿着损失函数梯度的相反方向移动（损   \t\t失函数值下降最快的方向）。\n​\t\t用下面的数学公式来表示这一更新过程（表示偏导数）：​\t\t总结一下，算法的步骤如下： \n​\t\t（1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。\n​\t  其中（batch_size）表示每个小批量中的样本数量，称为批量大小，表示学习率。批量大小和学习率的值通常是手动预先指定，而不是通过模型训\t练得到的。 \n​\t  这些可以调整但不在训练过程中更新的参数称为超参数。\n​\t\t调参即为选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估\t\t得到的。\n从0实现线性回归代码python# 从0开始实现线性回归代码\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch as d2l\n\n#生成数据集\ndef synthetic_data(w, b, num_examples):  \n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))      # 均值为0，方差为1，大小为：n个样本、w长度个特征\n    y = torch.matmul(X, w) + b                          # matmul函数为不支持广播机制的矩阵乘法\n    y += torch.normal(0, 0.01, y.shape)                 # 给y加一个随机噪声\n    return X, y.reshape((-1, 1))             #将y作为列向量返回\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\nprint('features:', features[0],'\\nlabel:', labels[0])\nfeatures: tensor([1.4632, 0.5511])     #第一个样本的各特征值\nlabel: tensor([5.2498])                #第一个样本的标签\n# 绘制样本    \nd2l.set_figsize()\nd2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);  #绘制所有样本第一列特征与标签的散点图\n\n\n#读取数据集（用于实现小批量抽样）\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)     #打乱indices中的数字顺序\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i: min(i + batch_size, num_examples)])      #每次取规模为batch_size的一些样本\n        yield features[batch_indices], labels[batch_indices]     #多次返回\n\n batch_size = 10\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break\ntensor([[ 0.3934,  2.5705],\n        [ 0.5849, -0.7124],\n        [ 0.1008,  0.6947],\n        [-0.4493, -0.9037],\n        [ 2.3104, -0.2798],\n        [-0.0173, -0.2552],\n        [ 0.1963, -0.5445],\n        [-1.0580, -0.5180],\n        [ 0.8417, -1.5547],\n        [-0.6316,  0.9732]])\n tensor([[-3.7623],\n        [ 7.7852],\n        [ 2.0443],\n        [ 6.3767],\n        [ 9.7776],\n        [ 5.0301],\n        [ 6.4541],\n        [ 3.8407],\n        [11.1396],\n        [-0.3836]])\n    \nw = torch.normal(0, 0.01, size=(2,1), requires_grad=True)    #均值为0，方差为0.01的2x1随机正态向量\nb = torch.zeros(1, requires_grad=True)             #偏移量b初始化为0\n\n#定义模型\ndef linreg(X, w, b):  \n    \"\"\"线性回归模型\"\"\"\n    return torch.matmul(X, w) + b\n\n#定义损失函数\ndef squared_loss(y_hat, y): \n    \"\"\"均方损失\"\"\"\n    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n\n#定义优化算法\ndef sgd(params, lr, batch_size): \n    \"\"\"小批量随机梯度下降\"\"\"\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size\n            param.grad.zero_()\n\n#训练\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)  # X和y的小批量损失\n        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，才是标量。\n        l.sum().backward()  # 计算关于[w,b]的梯度\n        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')​\t\t\n总结：线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。\n","slug":"Pytorch学习","date":"2024-01-31T05:51:09.000Z","categories_index":"深度学习","tags_index":"Pytorch","author_index":"Sonderlin"}]