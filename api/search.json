[{"id":"43114f1350cad18fe627afc10a1b1c7b","title":"Transcend：Detecting Concept Drift in Malware Classification Models论文阅读总结","content":"Transcend: Detecting Concept Drift in Malware Classification Models出处：USENIX Security Symposium\n时间：2017\n论文概述针对攻击者不断改良恶意软件带来的概念漂移问题，本文在模型性能下降之前就进行对退化的检测模型进行识别，从而及时完成检测模型的更新\n现有研究传统方法识别模型退化：\n\n根据模型检测完成后的准确率等指标后顾性判断\n计算测试对象在候选类别中的拟合概率\n\n解决方案Conformal evaluation（保形评估）置信度（Confidence）、可信度（Credibility）、不一致性度量都是保形评估的核心。\n不一致性度量\n\n其中评分函数AD即核心的不一致性度量函数\n本文中使用p值作为相似性的度量（P-values as a Similarity Metric）：\n\n计算式解释：\n\n\nCredibility（可信度）定义：表示样本与其被分类的类别在统计上的相似程度，即样本属于该类别的可信度。\n\n\n\n\n总结：可信度越高，则表明样本在统计上更接近被分类的类别。但存在多类别匹配的局限，即对于多个类别的p值都很高，因此引入置信度\nConfidence（置信度）定义：表示算法对给定分类决策的确信程度，即算法认为该决策正确的程度\n\n\nps：max括号中的意思是除了已选择的标签的p值，最终得到的是其他标签下的p值最大值\n决策评估\n\n如果一个算法的决策评估状态是出现高可信度且高置信度的，我们就认为这个算法在决策评估上目前还不错，出现其他情况就说明可能发生了概念漂移。\n","slug":"Transcend","date":"2024-08-30T05:51:09.000Z","categories_index":"机器学习","tags_index":"概念漂移,保形预测","author_index":"Sonderlin"},{"id":"de3a9296d5a1bb4a6f357efb2f755731","title":"Hexo博客配置时遇到的问题","content":"Hexo博客配置时遇到的问题问题1：hexo博客代码高亮失效吐血了。。。csdn真一坨XX。。。网上找了半天才发现是hexo的版本高于7.0.0时代码高亮就会失效。\nbashnpm i hexo@6.0.0 //回退版本至6.x.x后成功解决问题2：作者页下面的信息栏统计有误。。一个文章必须要先设置category再设置tags  两者才会统计生效\n问题3：新建文章时没有自带categories字段默认情况下，新创建的文章是没有 categories 和 tags 的，我们可以在根目录下的 scaffolds\\post.md 文件添加即可。这样每次新建文章，就自动有了这些参数。\n终于没问题了！！！准备开始愉快地写博客了！！\n","slug":"Hexo博客配置时遇到的问题","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客配置问题","author_index":"Sonderlin"},{"id":"9627999a85156d234bd83b92abfc393b","title":"Hexo博客发文/配置/管理常用指令","content":"Hexo博客发文&#x2F;配置&#x2F;管理常用指令powershellhexo init  //初始化\n\nhexo new/n (title)  //新建文章\n\nhexo generate/g  //生成静态文件\n\nhexo server/s   //启动本地服务器 进行预览\n\nhexo clean      //清理缓存，有时更改了设置需要清除缓存后才可生效\n\nhexo deploy    //将本地生成的静态文件部署到github仓库中上线","slug":"Hexo博客常用指令","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客管理命令","author_index":"Sonderlin"},{"id":"feee444778bd7ce32a76dc60cad27adf","title":"SHAP算法原理","content":"Shapley值1953年，Shapley值首次提出。Lloyd Shapley在论文《A Value for n-Person Games》中首次提出Shapley值。Shapley值是合作博弈论中的一个解决方案，它提供了一种在玩家之间公平分配支出的数学方法。从此成为联盟博弈论的基石，经常被用来确定团体内公平有效的资源分配策略，包括在股东之间分配利润、在合作者之间分配成本以及将功劳分配给研究项目的贡献者。然而，它与机器学习的交集要到五十年后才有。\nShapley值计算假设以下情形：已经训练了一个机器学习模型来预测公寓价格，分别有park、size、floor、car四个特征。\n某个面积为50平方米（size&#x3D;50）、位于二楼（floor&#x3D;2nd）、附近有一个公园（park&#x3D;nearby）、禁止猫咪（cat&#x3D;banned）的公寓，它预测价格为300,000欧元，你需要解释这个预测，即每个特征是如何促进预测的？当所有公寓的平均预测为310,000欧元时，与平均预测相比，每个特征值对预测的贡献是多少？\n线性回归模型的答案很简单，每个特征的贡献是特征的权重乘以特征值，但这仅适用于线性模型\npark&#x3D;nearby，cat&#x3D;banned，size&#x3D;50，floor&#x3D;2nd的特征值共同实现了300,000欧元的预测，而我们的目标是解释实际预测（300,000欧元）和平均预测（310,000欧元）之间的差异：-10,000欧元。\n答案可能是：park&#x3D;nearby贡献了30,000欧元，size&#x3D;50贡献了10,000欧元，floor&#x3D;2nd贡献了0欧元，cat&#x3D;banned贡献了-50,000欧元，这些贡献加起来为-10,000欧元，最终预测为平均预测加上该贡献之和。\n那么我们该如何计算目标公寓实例各特征的Shapley值呢？\n在下面中，我们估计了cat&#x3D;banned特征值被添加到park&#x3D;nearby和size&#x3D;50的联盟后的贡献。\n第一步，我们从数据中随机抽取另一个公寓（随机找一个floor&#x3D;1st的公寓），使用该公寓自己的floor特征值1st，模拟出park&#x3D;nearby，size&#x3D;50和cat&#x3D;banned的联盟，然后我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;banned）预测公寓的价格为310,000欧元，这个估计值取决于随机抽取的公寓的值，这称之为基线值。\n第二步，我们从联盟中删除cat&#x3D;banned，然后用该公寓的cat特征值allowed替代，我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;allowed）预测公寓的价格为320,000欧元。\n第三步，重复以上过程，floor取另一个值，直到找完，就找完了被添加到park&#x3D;nearby和size&#x3D;50的联盟后的全部贡献\n将以下全部联盟的贡献都找完，计算带有和不带有cat&#x3D;banned特征值的预测公寓价格，并计算差值来获得边际贡献，将边际贡献求出（加权）平均值，从而得出Shapley值。\n \n\n\n\n","slug":"Shapley","date":"2024-07-07T14:15:09.000Z","categories_index":"机器学习","tags_index":"SHAP","author_index":"Sonderlin"},{"id":"977ed608adc923ebbb0d8daa2e5720c8","title":"DOH隧道工具使用","content":"dns2tcp使用教程https://www.jianshu.com/p/9e8367fee777\n文件传输netcat工具安装https://blog.csdn.net/qq_50377269/article/details/135772156\n等待ns记录生效ing\n(开错端口了 开成tcp53了  重新开放udp53就可以了\ngodoh doh隧道工具https://github.com/sensepost/godoh\n","slug":"DOH隧道工具使用","date":"2024-07-07T06:05:49.000Z","categories_index":"DNS","tags_index":"DoH隧道","author_index":"Sonderlin"},{"id":"c723f7d5a5457cbc1c88bbea11b3a805","title":"Pytorch学习","content":"Pytorch学习数据操作tensor为torch中的一个数据结构，可以参与运算，称为张量，即多个数值组成的数组，可能有多个维度\npythonimport torch\nx = torch.arrange(3)  //生成一个从0开始,长度为3的一维tensor\nx\nOut:tensor([0,1,2])\n    \nx.shape                //查看tensor的形状\nOut:torch.Size([3])    //形状为：一维、长度为3\n    \nX=x.reshape(3,4)       //改变一个tensor的形状 改为二维的3x4矩阵\nOut:tensor([[],\n       ....            //一个二维矩阵\n           ])\n\ntorch.cat((tensor1,tensor2),dim=0/1)       //按行或按列 合并tensor\n\n\n//torch的广播机制\n当两个tensor维度相同，但形状不一致进行运算时，两者从低到高扩展然后进行运算\n\n//torch的原地操作（以节省内存）\nZ = torch.zeros_like(Y)   //形状与Y一致，但初值为全0\nprint(&#39;id(Z):&#39;, id(Z))\nZ[:] = X + Y\nprint(&#39;id(Z):&#39;, id(Z))\n\nid(Z): 140327634811696             \nid(Z): 140327634811696            //运算后地址不变，成功原地操作\n    \n//对于可变对象，如列表、集合、字典等\nX += Y 不等同于 X = X + Y\n前者为原地操作，后者创建了新的对象（浪费了内存）\n对于不可变对象 两者等同\n\n\n//torch与numpy\nx = torch.arrange(3)\nx为一个tensor对象\nA = x.numpy()  //将一个tensor对象转换为numpy中的ndarray对象\nB = torch.tensor(A)  //将一个ndarray对象转换为tensor对象\n\na = torch.tensor([3.5])   //创建一个一维的 大小为1 的张量\na, a.item(), float(a), int(a)    //转换为python中的标量\noutput:(tensor([3.5000]), 3.5, 3.5, 3)线性代数利用代码实现一些线代中的运算\npythonimport torch\nA = torch.arange(20).reshape(5, 4)     //初始化 5x4矩阵\nA\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\nA.T                                    //获取其转置\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n\n//矩阵加法\nA = torch.arange(20, dtype=torch.float32).reshape(5, 4)\nB = A.clone()  # 通过分配新内存，将A的一个副本分配给B\nA, A + B\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))\n\nA * B\ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])","slug":"Pytorch学习","date":"2024-01-31T05:51:09.000Z","categories_index":"机器学习","tags_index":"Pytorch,Numpy","author_index":"Sonderlin"}]