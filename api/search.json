[{"id":"0d2626f16e99b6aca31641a73ebbe19c","title":"对抗攻击3——Explaining and harnessing adversarial examples论文解读","content":"对抗攻击3——Explaining and harnessing adversarial examples论文解读论文概述以前的工作大多认为对抗样本的存在是由于深度神经网络的极端非线性，可能与模型纯监督学习下的过拟合、正则化不足问题有关。作者证明了这些推测是不必要的，因为作者认为神经网络容易受到对抗性扰动影响的主要原因是因为它的线性本质，并基于此提出了快速梯度符号法FGSM以及进行了相关实验。\n相关工作\nBox-constrained L-BFGS可以可靠地找到对抗样本。\n在一些数据集上，如ImageNet (Deng et al.， 2009)，对抗例子与原始例子非常接近，以至于人眼无法分辨出差异。\n同一对抗实例经常被具有不同架构的各种分类器或者在训练集的不同子集上训练的分类器错误分类。\n浅的softmax回归模型也容易受到敌对例子的影响。\n在对抗样本上进行训练可以正则化模型——然而，这在当时是不实际的，因为需要在内部循环中进行昂贵的约束优化。\n\n主要内容\n对抗样本的线性解释\n线性关系如何会产生对抗样本，考虑权重向量和对抗样本和对抗扰动的关系式如下：上式可以看出如果有维，且每一个权重向量的元素的均量维，那么扰动的增长便是mn线性增长的，因此对于高维的关系，即使输入加入很小的扰动，其结果也会产生较大的变化，从而解释了高维的复杂神级网络对抗样本的存在也是因为其线性本质。\n\n非线性模型的线性扰动\n作者认为神经网络是过于线性以至于不能抵抗线性对抗干扰，如ReLU等激活函数，为了优化计算速度，都有意地采用非常线性的形式，而这些线性行为所带来的对抗样本的脆弱性也将摧毁神经网络。\n​                                                                                       \n基于对抗样本的线性解释观点，作者提出了一种快速产生对抗样本的方法：快速梯度符号法FGSM，并进行了相关实验。\n假设是模型的参数，是输入，是输出，是神经网络的误差函数。作者通过在当前输入位置对非线性误差函数进行线性化操作（一阶泰勒展开），接着对线性化后的函数求关于输入x的梯度方向，再对其进行sign符号函数处理其分量，获取梯度的正向方向（沿梯度正向方向，损失函数值增加的最快，误差越高），再乘以常数以控制其扰动的幅度。x+即为最终得到的对抗样本\n以上作者通过实验证实，确实能够产生对抗样本，为对抗样本的线性解释提供有力的证据。\n\n线性模型的对抗训练\n对于逻辑回归模型，假如用其去识别标签：，其中就是逻辑sigmoid函数了。可以通过计算下面公式的梯度来训练模型：\n应用FGSM算法，即可得到对抗版本的逻辑回归损失函数：\n\n深度网络的对抗训练\n同样是基于FGSM算法改进损失函数：文中实验部分作者设置=0.5\n\n\n","slug":"Explaining and harnessing adversarial examples","date":"2024-09-06T10:28:09.000Z","categories_index":"机器学习","tags_index":"对抗样本","author_index":"Sonderlin"},{"id":"68873544f6b8e969a8c85b81e08aff1b","title":"对抗攻击2——Intriguing properties of neural networks论文解读","content":"对抗攻击2——Intriguing properties of neural networks论文解读符号说明\n\n论文概要本文主要提出了两个重要的观点：\n观点一：\n关于神经网络的语义分析，以前的研究工作持有这样的观点：神经网络的语义信息独立地 (individually) 保存在每一个神经元内，特别是在最后一个隐藏层中，每一个神经元都可以作为数据的一个语义特征 (semantic feature)。以上观点可以用如下式子表述：这个表达式的意思是找到图像x‘，使得φ(x)在方向的分量最大，也就是说图像x‘最突出地反映了ei分量所在的神经元代表的语义特征。找到许多满足式子的x‘，就可以总结出单个神经元所代表的语义特征。\n而作者不再取某个神经元所在的单位方向矢量，而是取向量空间里的一个随机单位矢量v，同样找到满足式子的图像集合。作者发现，这样找到的集合同样具有相似的语义特征，得到了同样的结果\n\n\n因此得出结论：神经网络的语义特征不存在于独立的神经元中，而存在于整个神经元激活的空间内。\n观点二：\n对抗样本的产生是低概率事件。据此我们可以推断，对抗样本在训练集和测试集中都很少，模型只学到非对抗样本的特征，所以当模型遇到与输入图像看起来“无异”的对抗样本时就会表现得非常脆弱。\n此外，作者提出了一种生成对抗样本的方法，称为Box-constrained L-BFGS。\n令 f 表示已训练好的神经网络，r 表示扰动信号，l 表示希望模型最终预测得到的类别，则需要优化的问题如下：\n\n\n可以得到对应的目标函数：\n\n上面的目标函数中分为两部分，第一部分是 c|r| 这一部分限制扰动信号r不能太大；第二部分是分类的损失函数loss，通过优化这个loss可以让神经网络把样本x+r预测为l类别的概率更大。\n下面是一部分实验结果，左边是原始图片，中间是扰动信号(为了更好的展示，图中扰动信号放大了10倍)，右边是增加了扰动信号后的图片。可以看到增加扰动信号后，图片肉眼看上去没什么区别，但是模型却预测错误了。\n\n\n作者在实验中还发现了对抗攻击一些有趣的方面：对于大部分网络结构和大部分样本，都可以生成一些肉眼很难区分的对抗样本迷惑神经网络。\n对抗样本有跨模型泛化能力：对于两个超参数不同的模型 1 和模型 2 (例如层数不同，每层神经元个数不同，正则化不同等) ，用模型 1生成的对抗样本，有大部分也可以让模型 2 预测错误。\n对抗样本有跨训练集泛化能力：将训练集划分为两个不同的子集，分别训练不同的模型 1 和模型 2，用模型 1 生成的对抗样本也可以提高模型2预测错误的概率。\n","slug":"Intriguing properties of neural networks论文解读","date":"2024-09-03T05:51:09.000Z","categories_index":"深度学习","tags_index":"对抗样本,图像识别","author_index":"Sonderlin"},{"id":"74bf40f03dab2977fa471e9edf464d2c","title":"对抗攻击1——Evasion attacks against machine learning at test time论文解读","content":"对抗攻击1——Evasion attacks against machine learning at test time论文解读论文概要采用梯度下降的方法生成对抗样本，并通过不断增加攻击者的先验知识与篡改能力来检验分类器的安全性能。并以恶意PDF文件检测系统入侵为例论证对抗样本的有效性。\n对手目标：创建（修改）一个样本，使得该样本被分类器以高置信度判别错误。例如，修改一个垃圾邮件样本，目标是使得分类器判别函数的值最小。\n对手知识：针对于待攻击系统（分类器），可能包括5个方面：a) 全部或者部分的训练集；b) 每个样本的特征表示；c) 学习算法和决策函数的类型；d) 已训练好的分类器模型（例如神经网络的参数）；e) 分类器的反馈（输入一个对抗样本，经过分类器得到label）。\n对手技能：只能改变测试集，不能修改训练集。包括以下3个方面：a) 修改输入数据b) 修改特征向量c) 独立修改某些特征\n攻击策略对于被分类器判别为垃圾邮件的样本 ，我们希望通过对其进行适当修改（在不改变邮件语义的前提下），使得分类器“尽可能相信”它是正常邮件样本。数学形式为：\n显然，若g(x)为凸函数，则很容易进行求解。但在非凸情况下，通常无法求得x的全局最优解。如上图中的左上图与右上图所示，沿梯度下降方向进行求解可能会向着数据分布稀疏的方向，在这些区域时，分类器给出的结果是不确定的，因此攻击不一定可以成功。\n要想增加攻击成功的概念，我们希望梯度下降方向是正常样本的密集区域，此时x可以向正常样本最密集的区域移动，攻击最有可能成功。换一种表述方式就是希望尽可能大一些，因此我们改写之前的式子，加入密度估计项来表述密集区域，得到下式：该式的梯度下降方向如上图中的下侧图所示，梯度下降方向 向着 正常样本集中的区域。从感性的角度理解，对抗样本是在模仿正常邮件的特征，使得自己的“外衣”越来越精致以迷惑分类器。\n补充知识核密度估计（Kernel Density Estimation, KDE）核密度估计是一种非参数方法，用于估计随机变量的概率密度函数。给定一个样本数据集{x1,x2,…,xn}，核密度估计的形式为：其中：\n\n是在点x处的概率密度估计值。\n是样本数据集的大小。\n是带宽参数，控制核函数的平滑度。\n是核函数，常见的核函数包括高斯核（RBF核）等，用于衡量样本与数据集中每个点之间的相似度。\n\n","slug":"Evasion attacks against machine learning at test time论文解读","date":"2024-09-01T05:51:09.000Z","categories_index":"机器学习","tags_index":"逃避攻击,对抗样本","author_index":"Sonderlin"},{"id":"43114f1350cad18fe627afc10a1b1c7b","title":"Transcend：Detecting Concept Drift in Malware Classification Models论文阅读总结","content":"Transcend: Detecting Concept Drift in Malware Classification Models出处：USENIX Security Symposium\n时间：2017\n论文概述针对攻击者不断改良恶意软件带来的概念漂移问题，本文在模型性能下降之前就进行对退化的检测模型进行识别，从而及时完成检测模型的更新\n现有研究传统方法识别模型退化：\n\n根据模型检测完成后的准确率等指标后顾性判断\n计算测试对象在候选类别中的拟合概率\n\n解决方案Conformal evaluation（保形评估）置信度（Confidence）、可信度（Credibility）、不一致性度量都是保形评估的核心。\n不一致性度量\n\n其中评分函数AD即核心的不一致性度量函数\n本文中使用p值作为相似性的度量（P-values as a Similarity Metric）：\n\n计算式解释：\n\n\nCredibility（可信度）定义：表示样本与其被分类的类别在统计上的相似程度，即样本属于该类别的可信度。\n\n\n\n\n总结：可信度越高，则表明样本在统计上更接近被分类的类别。但存在多类别匹配的局限，即对于多个类别的p值都很高，因此引入置信度\nConfidence（置信度）定义：表示算法对给定分类决策的确信程度，即算法认为该决策正确的程度\n\n\nps：max括号中的意思是除了已选择的标签的p值，最终得到的是其他标签下的p值最大值\n决策评估\n\n如果一个算法的决策评估状态是出现高可信度且高置信度的，我们就认为这个算法在决策评估上目前还不错，出现其他情况就说明可能发生了概念漂移。\n","slug":"Transcend","date":"2024-08-30T05:51:09.000Z","categories_index":"机器学习","tags_index":"概念漂移,保形预测","author_index":"Sonderlin"},{"id":"9627999a85156d234bd83b92abfc393b","title":"Hexo博客发文/配置/管理常用指令","content":"Hexo博客发文&#x2F;配置&#x2F;管理常用指令powershellhexo init  //初始化\n\nhexo new/n (title)  //新建文章\n\nhexo generate/g  //生成静态文件\n\nhexo server/s   //启动本地服务器 进行预览\n\nhexo clean      //清理缓存，有时更改了设置需要清除缓存后才可生效\n\nhexo deploy    //将本地生成的静态文件部署到github仓库中上线","slug":"Hexo博客常用指令","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客管理命令","author_index":"Sonderlin"},{"id":"de3a9296d5a1bb4a6f357efb2f755731","title":"Hexo博客配置时遇到的问题","content":"Hexo博客配置时遇到的问题问题1：hexo博客代码高亮失效吐血了。。。csdn真一坨XX。。。网上找了半天才发现是hexo的版本高于7.0.0时代码高亮就会失效。\nbashnpm i hexo@6.0.0 //回退版本至6.x.x后成功解决问题2：作者页下面的信息栏统计有误。。一个文章必须要先设置category再设置tags  两者才会统计生效\n问题3：新建文章时没有自带categories字段默认情况下，新创建的文章是没有 categories 和 tags 的，我们可以在根目录下的 scaffolds\\post.md 文件添加即可。这样每次新建文章，就自动有了这些参数。\n终于没问题了！！！准备开始愉快地写博客了！！\n","slug":"Hexo博客配置时遇到的问题","date":"2024-08-28T12:14:59.000Z","categories_index":"博客管理","tags_index":"博客配置问题","author_index":"Sonderlin"},{"id":"feee444778bd7ce32a76dc60cad27adf","title":"SHAP算法原理","content":"Shapley值1953年，Shapley值首次提出。Lloyd Shapley在论文《A Value for n-Person Games》中首次提出Shapley值。Shapley值是合作博弈论中的一个解决方案，它提供了一种在玩家之间公平分配支出的数学方法。从此成为联盟博弈论的基石，经常被用来确定团体内公平有效的资源分配策略，包括在股东之间分配利润、在合作者之间分配成本以及将功劳分配给研究项目的贡献者。然而，它与机器学习的交集要到五十年后才有。\nShapley值计算假设以下情形：已经训练了一个机器学习模型来预测公寓价格，分别有park、size、floor、car四个特征。\n某个面积为50平方米（size&#x3D;50）、位于二楼（floor&#x3D;2nd）、附近有一个公园（park&#x3D;nearby）、禁止猫咪（cat&#x3D;banned）的公寓，它预测价格为300,000欧元，你需要解释这个预测，即每个特征是如何促进预测的？当所有公寓的平均预测为310,000欧元时，与平均预测相比，每个特征值对预测的贡献是多少？\n线性回归模型的答案很简单，每个特征的贡献是特征的权重乘以特征值，但这仅适用于线性模型\npark&#x3D;nearby，cat&#x3D;banned，size&#x3D;50，floor&#x3D;2nd的特征值共同实现了300,000欧元的预测，而我们的目标是解释实际预测（300,000欧元）和平均预测（310,000欧元）之间的差异：-10,000欧元。\n答案可能是：park&#x3D;nearby贡献了30,000欧元，size&#x3D;50贡献了10,000欧元，floor&#x3D;2nd贡献了0欧元，cat&#x3D;banned贡献了-50,000欧元，这些贡献加起来为-10,000欧元，最终预测为平均预测加上该贡献之和。\n那么我们该如何计算目标公寓实例各特征的Shapley值呢？\n在下面中，我们估计了cat&#x3D;banned特征值被添加到park&#x3D;nearby和size&#x3D;50的联盟后的贡献。\n第一步，我们从数据中随机抽取另一个公寓（随机找一个floor&#x3D;1st的公寓），使用该公寓自己的floor特征值1st，模拟出park&#x3D;nearby，size&#x3D;50和cat&#x3D;banned的联盟，然后我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;banned）预测公寓的价格为310,000欧元，这个估计值取决于随机抽取的公寓的值，这称之为基线值。\n第二步，我们从联盟中删除cat&#x3D;banned，然后用该公寓的cat特征值allowed替代，我们用这个组合（floor&#x3D;1st，park&#x3D;nearby，size&#x3D;50，cat&#x3D;allowed）预测公寓的价格为320,000欧元。\n第三步，重复以上过程，floor取另一个值，直到找完，就找完了被添加到park&#x3D;nearby和size&#x3D;50的联盟后的全部贡献\n将以下全部联盟的贡献都找完，计算带有和不带有cat&#x3D;banned特征值的预测公寓价格，并计算差值来获得边际贡献，将边际贡献求出（加权）平均值，从而得出Shapley值。\n \n\n\n\n","slug":"Shapley","date":"2024-07-07T14:15:09.000Z","categories_index":"机器学习","tags_index":"SHAP","author_index":"Sonderlin"},{"id":"977ed608adc923ebbb0d8daa2e5720c8","title":"DOH隧道工具使用","content":"dns2tcp使用教程https://www.jianshu.com/p/9e8367fee777\n文件传输netcat工具安装https://blog.csdn.net/qq_50377269/article/details/135772156\n等待ns记录生效ing\n(开错端口了 开成tcp53了  重新开放udp53就可以了\ngodoh doh隧道工具https://github.com/sensepost/godoh\n","slug":"DOH隧道工具使用","date":"2024-07-07T06:05:49.000Z","categories_index":"DNS","tags_index":"DoH隧道","author_index":"Sonderlin"},{"id":"c723f7d5a5457cbc1c88bbea11b3a805","title":"Pytorch学习","content":"Pytorch学习数据操作tensor为torch中的一个数据结构，可以参与运算，称为张量，即多个数值组成的数组，可能有多个维度\npythonimport torch\nx = torch.arrange(3)  #生成一个从0开始,长度为3的一维tensor\nx\nOut:tensor([0,1,2])\n    \nx.shape                #查看tensor的形状\nOut:torch.Size([3])    #形状为：一维、长度为3\n    \nX=x.reshape(3,4)       #改变一个tensor的形状 改为二维的3x4矩阵\nOut:tensor([[],\n       ....            #一个二维矩阵\n           ])\n\ntorch.cat((tensor1,tensor2),dim=0/1)       #按行或按列 合并tensor\n\n\n#torch的广播机制\n当两个tensor维度相同，但形状不一致进行运算时，两者从低到高扩展然后进行运算\n\n#torch的原地操作（以节省内存）\nZ = torch.zeros_like(Y)   #形状与Y一致，但初值为全0\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n\nid(Z): 140327634811696             \nid(Z): 140327634811696            #运算后地址不变，成功原地操作\n    \n#对于可变对象，如列表、集合、字典等\nX += Y 不等同于 X = X + Y\n前者为原地操作，后者创建了新的对象（浪费了内存）\n对于不可变对象 两者等同\n\n\n#torch与numpy\nx = torch.arrange(3)\nx为一个tensor对象\nA = x.numpy()  #将一个tensor对象转换为numpy中的ndarray对象\nB = torch.tensor(A)  #将一个ndarray对象转换为tensor对象\n\na = torch.tensor([3.5])   #创建一个一维的 大小为1 的张量\na, a.item(), float(a), int(a)    #转换为python中的标量\noutput:(tensor([3.5000]), 3.5, 3.5, 3)线性代数利用代码实现一些线代中的运算\npythonimport torch\nA = torch.arange(20).reshape(5, 4)     #初始化 5x4矩阵\nA\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\nA.T                                    #获取其转置\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n\n#矩阵加法\nA = torch.arange(20, dtype=torch.float32).reshape(5, 4)\nB = A.clone()  # 通过分配新内存，将A的一个副本分配给B\nA, A + B\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))\n\nA * B                        #按元素乘法  \ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\n\n#降维操作\nx = torch.arange(4, dtype=torch.float32)\nx, x.sum()\n(tensor([0., 1., 2., 3.]), tensor(6.))\n\n#默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。\nA_sum_axis0 = A.sum(axis=0)\nA_sum_axis0, A_sum_axis0.shape\n(tensor([40., 45., 50., 55.]), torch.Size([4]))\nA_sum_axis1 = A.sum(axis=1)\nA_sum_axis1, A_sum_axis1.shape\n(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))\n\n#求平均值\nA.mean(), A.sum() / A.numel()\n(tensor(9.5000), tensor(9.5000))\n\n#点积\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\n\n#矩阵-向量积\nA.shape, x.shape, torch.mv(A, x)\n(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))\n\n#矩阵-矩阵乘法\nB = torch.ones(4, 3)\ntorch.mm(A, B)\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n\n#范数\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)             #L2范数：所有元素平方和的平方根\ntensor(5.)\n\ntorch.abs(u).sum()        #L1范数：所有元素的绝对值求和\ntensor(7.)微积分对矩阵进行诸如求导数等运算\n\n\n分子布局符号，以分子优先\n对矩阵求导后的形状：\n\n\n计算图理论\n\n\n由此引出反向传递：\n\n\n\n\n\n\n可以发现反向传播需要利用正向传播求导的中间结果，空间复杂度较高（因此比较占用GPU资源），计算复杂度与正向传播的代价类似，但正向传播不需要存储中间结果，空间复杂度较低，但正是由于其不存储中间结果，因此导致每次都需要重新计算，导致在实际使用时的时间复杂度太高。\n自动求导python import torch\n\nx = torch.arange(4.0)\nx\ntensor([0., 1., 2., 3.])\n\nx.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\nx.grad  # 默认值是None\n\n# 现在计算y\ny = 2 * torch.dot(x, x)         #内积，因此y=2x^2\ny\ntensor(28., grad_fn=&lt;MulBackward0&gt;)\n\n#接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\ny.backward()      #反向传播进行求导\nx.grad            #直接访问y关于x的导数\n\nx.grad == 4 * x\ntensor([True, True, True, True])\n\nx.grad.zero_()         # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n\ny = x.sum()\ny.backward()\nx.grad\ntensor([1., 1., 1., 1.])\n\n#在深度学习中 一般仅对标量进行求导\n\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()     #清空梯度\ny = x * x\n# 等价于y.backward(torch.ones(len(x)))\ny.sum().backward()\nx.grad\n\n\nx.grad.zero_()\ny = x * x\nu = y.detach()     #当做常数\nz = u * x   \n\nz.sum().backward()      \nx.grad == u        #求导时也将其当做了常数\ntensor([True, True, True, True])\n\n\n#控制流的梯度计算\ndef f(a):\n    b = a * 2\n    while b.norm() &lt; 1000:\n        b = b * 2\n    if b.sum() &gt; 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\na.grad == d / a\ntensor(True)线性神经网络线性回归​\t\t线性回归是指目标可以表示为特征的加权和，如下面的式子，其中w代表权重，x代表某样本数据的各特征，b偏移量（所有特征为0时的值）：​       我们可以用点积形式来简洁地表达模型：其中向量x对应于单个数据样本的特征​\t\t对于特征集合X，预测值可以通过矩阵-向量乘法表示为：其中，X的每一行是一个样本，每一列是一种特征。​\t\t在开始寻找最好的模型参数（model parameters）和之前， 我们还需要两个东西：一种模型质量的度量方式；一种能够更新模型以提高模型预测质量的方法。\n损失函数​\t在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。​\t损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。当样本i的预测值为，其相应的真实标签为时， 平方误差可以定义为以下公式：   为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）。   在训练模型时，我们希望寻找一组参数（w∗,b∗）， 这组参数能最小化在所有训练样本上的总损失。如下式：   我们的预测问题是最小化，由于其为凸函数，即在损失平面上只有一个临界点，所以存在参数的解析解。因此将损失关于w的导数设为0，得到解析解：\n   像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。\n随机梯度下降即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。\n​\t\t本书中我们用到一种名为梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向\t\t上更新参数来降低误差。\n​\t\t梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常\t\t慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量\t\t随机梯度下降（minibatch stochastic gradient descent）。\n​       在每次迭代中，我们首先随机抽样一个小批量，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以\t\t称为梯度）。最后，我们将梯度乘以一个预先确定的正数（即学习率），并从当前参数的值中减掉，从而将参数沿着损失函数梯度的相反方向移动（损   \t\t失函数值下降最快的方向）。\n​\t\t用下面的数学公式来表示这一更新过程（表示偏导数）：​\t\t总结一下，算法的步骤如下： \n​\t\t（1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。\n​\t  其中（batch_size）表示每个小批量中的样本数量，称为批量大小，表示学习率。批量大小和学习率的值通常是手动预先指定，而不是通过模型训\t练得到的。 \n​\t  这些可以调整但不在训练过程中更新的参数称为超参数。\n​\t\t调参即为选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估\t\t得到的。\n从0实现线性回归代码python# 从0开始实现线性回归代码\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch as d2l\n\n#生成数据集\ndef synthetic_data(w, b, num_examples):  \n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))      # 均值为0，方差为1，大小为：n个样本、w长度个特征\n    y = torch.matmul(X, w) + b                          # matmul函数为不支持广播机制的矩阵乘法\n    y += torch.normal(0, 0.01, y.shape)                 # 给y加一个随机噪声\n    return X, y.reshape((-1, 1))             #将y作为列向量返回\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\nprint('features:', features[0],'\\nlabel:', labels[0])\nfeatures: tensor([1.4632, 0.5511])     #第一个样本的各特征值\nlabel: tensor([5.2498])                #第一个样本的标签\n# 绘制样本    \nd2l.set_figsize()\nd2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);  #绘制所有样本第一列特征与标签的散点图\n\n\n#读取数据集（用于实现小批量抽样）\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)     #打乱indices中的数字顺序\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i: min(i + batch_size, num_examples)])      #每次取规模为batch_size的一些样本\n        yield features[batch_indices], labels[batch_indices]     #多次返回\n\n batch_size = 10\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break\ntensor([[ 0.3934,  2.5705],\n        [ 0.5849, -0.7124],\n        [ 0.1008,  0.6947],\n        [-0.4493, -0.9037],\n        [ 2.3104, -0.2798],\n        [-0.0173, -0.2552],\n        [ 0.1963, -0.5445],\n        [-1.0580, -0.5180],\n        [ 0.8417, -1.5547],\n        [-0.6316,  0.9732]])\n tensor([[-3.7623],\n        [ 7.7852],\n        [ 2.0443],\n        [ 6.3767],\n        [ 9.7776],\n        [ 5.0301],\n        [ 6.4541],\n        [ 3.8407],\n        [11.1396],\n        [-0.3836]])\n    \nw = torch.normal(0, 0.01, size=(2,1), requires_grad=True)    #均值为0，方差为0.01的2x1随机正态向量\nb = torch.zeros(1, requires_grad=True)             #偏移量b初始化为0\n\n#定义模型\ndef linreg(X, w, b):  \n    \"\"\"线性回归模型\"\"\"\n    return torch.matmul(X, w) + b\n\n#定义损失函数\ndef squared_loss(y_hat, y): \n    \"\"\"均方损失\"\"\"\n    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n\n#定义优化算法\ndef sgd(params, lr, batch_size): \n    \"\"\"小批量随机梯度下降\"\"\"\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size\n            param.grad.zero_()\n\n#训练\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)  # X和y的小批量损失\n        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，才是标量。\n        l.sum().backward()  # 计算关于[w,b]的梯度\n        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')​\t\t\n总结：线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。\nSoftmax回归回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”。\n通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 \n这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。\n分类标签接下来，我们要选择如何表示标签。一般的分类问题并不与类别之间的自然顺序有关。幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。  独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。在我们的例子中，标签y将是一个三维向量， 其中(1, 0, 0)对应于“猫”、(0,1,0)对应于“鸡”、(0,0,1)对应于“狗”。\n网络架构为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。\nsoftmax运算然而我们能否将未规范化的预测直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。\n为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：其中这里，对于所有的j总有。因此，可以视为一个正确的概率分布。 \n此外，由于softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。\n损失函数通过比较真实概率与估计概率之间的区别作为损失，而交叉熵常用于衡量两个概率的区别，因此我们将其作为损失：$$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}j.对其求关于未规范化的预测的导数，我们得到：\\partial{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.$$换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 \n从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值和估计值之间的差异。\n","slug":"Pytorch学习","date":"2024-01-31T05:51:09.000Z","categories_index":"深度学习","tags_index":"Pytorch","author_index":"Sonderlin"}]